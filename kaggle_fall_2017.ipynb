{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Fall 2017 Kaggle Solution**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions\n",
    "\n",
    "Same functions as in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode and Setup Network\n",
    "\n",
    "These functions define the neural network I will use and how I will encode the feature vector. Also the BATCH_SIZE and MAX_EPOCHS are defined here.  I craft this area to get better scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_EPOCHS = 158\n",
    "\n",
    "def encode(df):    \n",
    "    \n",
    "    # Feature engineering\n",
    "    df['density'] = df['population'] / df['sqmiles']    \n",
    "    df['dcomp'] = 0 \n",
    "    df.loc[df['type']==0,'dcomp'] = df.loc[df['type']==0,'0_type_count']\n",
    "    df.loc[df['type']==1,'dcomp'] = df.loc[df['type']==1,'1_type_count']\n",
    "    df.loc[df['type']==2,'dcomp'] = df.loc[df['type']==2,'2_type_count']\n",
    "    df.loc[df['type']==3,'dcomp'] = df.loc[df['type']==3,'3_type_count']\n",
    "    df.loc[df['type']==4,'dcomp'] = df.loc[df['type']==4,'4_type_count']\n",
    "    df['wealth'] = (df['income'] * df['population'])/1000\n",
    "    df['tcomp'] = df['0_type_count']+df['1_type_count']+df['2_type_count']+df['3_type_count']+df['4_type_count']\n",
    "    df['junk'] = np.random.uniform(size=len(df))\n",
    "    \n",
    "    # Encoding\n",
    "    encode_numeric_zscore(df,'age') \n",
    "    encode_numeric_zscore(df,'sqft') \n",
    "    encode_numeric_zscore(df,'income')\n",
    "    encode_numeric_zscore(df,'lot_size')\n",
    "    encode_numeric_zscore(df,'pets')\n",
    "    encode_numeric_zscore(df,'population')\n",
    "    encode_numeric_zscore(df,'sqmiles')\n",
    "    encode_numeric_zscore(df,'urban')\n",
    "    encode_numeric_zscore(df,'density')\n",
    "    encode_numeric_zscore(df,'wealth')\n",
    "    encode_numeric_zscore(df,'zip') # bad idea, but I want to show where it ranks\n",
    "    encode_text_dummy(df,'type_name')\n",
    "    encode_numeric_zscore(df,'dcomp')\n",
    "    \n",
    "    # Feature selection\n",
    "    df.drop('type', axis=1, inplace=True)\n",
    "    df.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    #df.drop('sqft', axis=1, inplace=True)\n",
    "    #df.drop('wealth', axis=1, inplace=True)\n",
    "    #df.drop('density', axis=1, inplace=True)\n",
    "    #df.drop('sqmiles', axis=1, inplace=True)\n",
    "    #df.drop('population', axis=1, inplace=True)\n",
    "    #df.drop('dcomp', axis=1, inplace=True)\n",
    "    #df.drop('urban', axis=1, inplace=True)##\n",
    "    df.drop('income', axis=1, inplace=True)\n",
    "    df.drop('tcomp', axis=1, inplace=True)\n",
    "    df.drop('1_type_count', axis=1, inplace=True)\n",
    "    df.drop('0_type_count', axis=1, inplace=True)\n",
    "    df.drop('3_type_count', axis=1, inplace=True)\n",
    "    df.drop('2_type_count', axis=1, inplace=True)\n",
    "    df.drop('lot_size', axis=1, inplace=True)\n",
    "    df.drop('zip', axis=1, inplace=True)\n",
    "    df.drop('pets', axis=1, inplace=True)\n",
    "    df.drop('age', axis=1, inplace=True)\n",
    "    df.drop('junk', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    if 'sales' in df.columns:\n",
    "        df.drop(df[df.sales == 0].index, inplace=True)\n",
    "    \n",
    "def build_network(x):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.1))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "I will evaluate feature importance, usually over 100 epochs, several times.  Each time I feature select (remove features) or engineer new ones, I will run at least one fold of the CV section, just to see what my validation error is.  This lets me get a rough idea of if I am helping or hurting anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "def perturbation_rank(model, x, y, names, regression):\n",
    "    errors = []\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        hold = np.array(x[:, i])\n",
    "        np.random.shuffle(x[:, i])\n",
    "        \n",
    "        if regression:\n",
    "            pred = model.predict(x)\n",
    "            error = metrics.mean_squared_error(y, pred)\n",
    "        else:\n",
    "            pred = model.predict_proba(x)\n",
    "            error = metrics.log_loss(y, pred)\n",
    "            \n",
    "        errors.append(error)\n",
    "        x[:, i] = hold\n",
    "        \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "\n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data, columns = ['name','error','importance'])\n",
    "    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n",
    "    result.reset_index(inplace=True, drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name np_utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8582cf48ff8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name np_utils"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./\"\n",
    "\n",
    "\n",
    "filename_train = os.path.join(path,\"preprocess_train.csv\")\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA'])\n",
    "\n",
    "# Preprocess\n",
    "# encode(df_train)\n",
    "\n",
    "\n",
    "# # Encode to a 2D matrix for training\n",
    "# x_train,y_train = to_xy(df_train,'sales')\n",
    "\n",
    "# # Fit on entire training\n",
    "# model = build_network(x_train)\n",
    "# model.fit(x_train,y_train,batch_size=BATCH_SIZE,verbose=2,epochs=MAX_EPOCHS)\n",
    "    \n",
    "# # Predict\n",
    "# pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_name-grocery</td>\n",
       "      <td>2.884399</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sqft</td>\n",
       "      <td>1.832996</td>\n",
       "      <td>0.635486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wealth</td>\n",
       "      <td>0.533841</td>\n",
       "      <td>0.185079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_name-electronics</td>\n",
       "      <td>0.532955</td>\n",
       "      <td>0.184772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type_name-outdoors</td>\n",
       "      <td>0.462659</td>\n",
       "      <td>0.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_name-farm</td>\n",
       "      <td>0.455983</td>\n",
       "      <td>0.158086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>density</td>\n",
       "      <td>0.334284</td>\n",
       "      <td>0.115894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_name-hardware</td>\n",
       "      <td>0.333903</td>\n",
       "      <td>0.115762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sqmiles</td>\n",
       "      <td>0.232331</td>\n",
       "      <td>0.080548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>population</td>\n",
       "      <td>0.171833</td>\n",
       "      <td>0.059573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dcomp</td>\n",
       "      <td>0.153476</td>\n",
       "      <td>0.053209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4_type_count</td>\n",
       "      <td>0.149442</td>\n",
       "      <td>0.051810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>urban</td>\n",
       "      <td>0.125971</td>\n",
       "      <td>0.043673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>income</td>\n",
       "      <td>0.124881</td>\n",
       "      <td>0.043295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0_type_count</td>\n",
       "      <td>0.119494</td>\n",
       "      <td>0.041428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1_type_count</td>\n",
       "      <td>0.118921</td>\n",
       "      <td>0.041229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tcomp</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.040494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3_type_count</td>\n",
       "      <td>0.110609</td>\n",
       "      <td>0.038347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2_type_count</td>\n",
       "      <td>0.108032</td>\n",
       "      <td>0.037454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>age</td>\n",
       "      <td>0.106182</td>\n",
       "      <td>0.036813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>zip</td>\n",
       "      <td>0.106015</td>\n",
       "      <td>0.036755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lot_size</td>\n",
       "      <td>0.105890</td>\n",
       "      <td>0.036711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>junk</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>0.036698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pets</td>\n",
       "      <td>0.105642</td>\n",
       "      <td>0.036625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name     error  importance\n",
       "0       type_name-grocery  2.884399    1.000000\n",
       "1                    sqft  1.832996    0.635486\n",
       "2                  wealth  0.533841    0.185079\n",
       "3   type_name-electronics  0.532955    0.184772\n",
       "4      type_name-outdoors  0.462659    0.160400\n",
       "5          type_name-farm  0.455983    0.158086\n",
       "6                 density  0.334284    0.115894\n",
       "7      type_name-hardware  0.333903    0.115762\n",
       "8                 sqmiles  0.232331    0.080548\n",
       "9              population  0.171833    0.059573\n",
       "10                  dcomp  0.153476    0.053209\n",
       "11           4_type_count  0.149442    0.051810\n",
       "12                  urban  0.125971    0.043673\n",
       "13                 income  0.124881    0.043295\n",
       "14           0_type_count  0.119494    0.041428\n",
       "15           1_type_count  0.118921    0.041229\n",
       "16                  tcomp  0.116800    0.040494\n",
       "17           3_type_count  0.110609    0.038347\n",
       "18           2_type_count  0.108032    0.037454\n",
       "19                    age  0.106182    0.036813\n",
       "20                    zip  0.106015    0.036755\n",
       "21               lot_size  0.105890    0.036711\n",
       "22                   junk  0.105851    0.036698\n",
       "23                   pets  0.105642    0.036625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is my orig ranking, with no feature selection (removing low ranked columns)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df_train.columns.values) # x column names\n",
    "names.remove('sales') # Remove the target\n",
    "rank = perturbation_rank(model, x_train, y_train, names, True)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>error</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type_name-grocery</td>\n",
       "      <td>2.814895</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sqft</td>\n",
       "      <td>2.045130</td>\n",
       "      <td>0.726539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wealth</td>\n",
       "      <td>0.654666</td>\n",
       "      <td>0.232572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>density</td>\n",
       "      <td>0.534584</td>\n",
       "      <td>0.189913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sqmiles</td>\n",
       "      <td>0.520010</td>\n",
       "      <td>0.184735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type_name-hardware</td>\n",
       "      <td>0.374401</td>\n",
       "      <td>0.133007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>type_name-electronics</td>\n",
       "      <td>0.311244</td>\n",
       "      <td>0.110570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>type_name-outdoors</td>\n",
       "      <td>0.290620</td>\n",
       "      <td>0.103244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>type_name-farm</td>\n",
       "      <td>0.199498</td>\n",
       "      <td>0.070872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>population</td>\n",
       "      <td>0.155126</td>\n",
       "      <td>0.055109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4_type_count</td>\n",
       "      <td>0.133952</td>\n",
       "      <td>0.047587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dcomp</td>\n",
       "      <td>0.116630</td>\n",
       "      <td>0.041433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>urban</td>\n",
       "      <td>0.115451</td>\n",
       "      <td>0.041014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name     error  importance\n",
       "0       type_name-grocery  2.814895    1.000000\n",
       "1                    sqft  2.045130    0.726539\n",
       "2                  wealth  0.654666    0.232572\n",
       "3                 density  0.534584    0.189913\n",
       "4                 sqmiles  0.520010    0.184735\n",
       "5      type_name-hardware  0.374401    0.133007\n",
       "6   type_name-electronics  0.311244    0.110570\n",
       "7      type_name-outdoors  0.290620    0.103244\n",
       "8          type_name-farm  0.199498    0.070872\n",
       "9              population  0.155126    0.055109\n",
       "10           4_type_count  0.133952    0.047587\n",
       "11                  dcomp  0.116630    0.041433\n",
       "12                  urban  0.115451    0.041014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is my new ranking, with feature selection (removing low ranked columns)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "names = list(df_train.columns.values) # x column names\n",
    "names.remove('sales') # Remove the target\n",
    "rank = perturbation_rank(model, x_train, y_train, names, True)\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossvalidate\n",
    "\n",
    "This section has two purposes:\n",
    "\n",
    "* I run this initially to determine if cutting features or engineering others is helping or hurting.\n",
    "* I run this again to get an idea of what my CV score is (estimate of my Kaggle position) and to see how many epochs I might want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Epoch 00147: early stopping\n",
      "Fold score (RMSE): 0.26310887932777405\n",
      "Fold #2\n",
      "Epoch 00184: early stopping\n",
      "Fold score (RMSE): 0.24052371084690094\n",
      "Fold #3\n",
      "Epoch 00147: early stopping\n",
      "Fold score (RMSE): 0.2813667356967926\n",
      "Fold #4\n",
      "Epoch 00184: early stopping\n",
      "Fold score (RMSE): 0.2582513093948364\n",
      "Fold #5\n",
      "Epoch 00132: early stopping\n",
      "Fold score (RMSE): 0.269981324672699\n",
      "Final, out of sample score (RMSE): 0.26299387216567993\n",
      "Fold early stop epochs: [147, 184, 147, 184, 132]\n",
      "Mean epochs: 158\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "MAX_EPOCH = 1000\n",
    "\n",
    "filename_read = os.path.join(path,\"train.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Preprocess\n",
    "encode(df)\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'sales')\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "epochs = []\n",
    "\n",
    "# Cross validate\n",
    "kf = KFold(5)\n",
    "\n",
    "fold = 0\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = build_network(x)\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50, verbose=1, mode='auto')\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],batch_size=BATCH_SIZE,\n",
    "              verbose=0,epochs=MAX_EPOCH)\n",
    "    \n",
    "    if monitor.stopped_epoch == 0:\n",
    "        epochs.append(MAX_EPOCH)\n",
    "    else:\n",
    "        epochs.append(monitor.stopped_epoch)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print(\"Final, out of sample score (RMSE): {}\".format(score))    \n",
    "\n",
    "print(\"Fold early stop epochs: {}\".format(epochs))\n",
    "print(\"Mean epochs: {}\".format(int(np.mean(epochs))))\n",
    "\n",
    "# 0.2790030837059021 - 9 features\n",
    "# 0.2442721426486969 - 7 features\n",
    "# 0.2635799050331116 - 5 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit\n",
    "\n",
    "Finally, generate a submission.  Train on the entire set, for the number of epochs that the CV indicated to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/158\n",
      "7s - loss: 0.4290\n",
      "Epoch 2/158\n",
      "7s - loss: 0.2051\n",
      "Epoch 3/158\n",
      "7s - loss: 0.1629\n",
      "Epoch 4/158\n",
      "7s - loss: 0.1473\n",
      "Epoch 5/158\n",
      "7s - loss: 0.1376\n",
      "Epoch 6/158\n",
      "7s - loss: 0.1285\n",
      "Epoch 7/158\n",
      "7s - loss: 0.1267\n",
      "Epoch 8/158\n",
      "7s - loss: 0.1219\n",
      "Epoch 9/158\n",
      "7s - loss: 0.1188\n",
      "Epoch 10/158\n",
      "7s - loss: 0.1185\n",
      "Epoch 11/158\n",
      "7s - loss: 0.1179\n",
      "Epoch 12/158\n",
      "7s - loss: 0.1120\n",
      "Epoch 13/158\n",
      "7s - loss: 0.1124\n",
      "Epoch 14/158\n",
      "7s - loss: 0.1108\n",
      "Epoch 15/158\n",
      "7s - loss: 0.1145\n",
      "Epoch 16/158\n",
      "7s - loss: 0.1126\n",
      "Epoch 17/158\n",
      "7s - loss: 0.1094\n",
      "Epoch 18/158\n",
      "7s - loss: 0.1092\n",
      "Epoch 19/158\n",
      "7s - loss: 0.1074\n",
      "Epoch 20/158\n",
      "7s - loss: 0.1070\n",
      "Epoch 21/158\n",
      "7s - loss: 0.1046\n",
      "Epoch 22/158\n",
      "7s - loss: 0.1068\n",
      "Epoch 23/158\n",
      "7s - loss: 0.1030\n",
      "Epoch 24/158\n",
      "7s - loss: 0.1035\n",
      "Epoch 25/158\n",
      "7s - loss: 0.1040\n",
      "Epoch 26/158\n",
      "7s - loss: 0.1055\n",
      "Epoch 27/158\n",
      "7s - loss: 0.1012\n",
      "Epoch 28/158\n",
      "7s - loss: 0.1007\n",
      "Epoch 29/158\n",
      "7s - loss: 0.1026\n",
      "Epoch 30/158\n",
      "7s - loss: 0.1018\n",
      "Epoch 31/158\n",
      "7s - loss: 0.1012\n",
      "Epoch 32/158\n",
      "7s - loss: 0.1014\n",
      "Epoch 33/158\n",
      "7s - loss: 0.1022\n",
      "Epoch 34/158\n",
      "7s - loss: 0.0999\n",
      "Epoch 35/158\n",
      "7s - loss: 0.1034\n",
      "Epoch 36/158\n",
      "7s - loss: 0.0974\n",
      "Epoch 37/158\n",
      "7s - loss: 0.1002\n",
      "Epoch 38/158\n",
      "7s - loss: 0.1000\n",
      "Epoch 39/158\n",
      "7s - loss: 0.0978\n",
      "Epoch 40/158\n",
      "7s - loss: 0.0986\n",
      "Epoch 41/158\n",
      "7s - loss: 0.0985\n",
      "Epoch 42/158\n",
      "7s - loss: 0.0979\n",
      "Epoch 43/158\n",
      "7s - loss: 0.0963\n",
      "Epoch 44/158\n",
      "7s - loss: 0.0959\n",
      "Epoch 45/158\n",
      "7s - loss: 0.0949\n",
      "Epoch 46/158\n",
      "7s - loss: 0.0958\n",
      "Epoch 47/158\n",
      "7s - loss: 0.0962\n",
      "Epoch 48/158\n",
      "7s - loss: 0.0944\n",
      "Epoch 49/158\n",
      "7s - loss: 0.0945\n",
      "Epoch 50/158\n",
      "7s - loss: 0.0932\n",
      "Epoch 51/158\n",
      "7s - loss: 0.0955\n",
      "Epoch 52/158\n",
      "7s - loss: 0.0932\n",
      "Epoch 53/158\n",
      "7s - loss: 0.0915\n",
      "Epoch 54/158\n",
      "7s - loss: 0.0946\n",
      "Epoch 55/158\n",
      "7s - loss: 0.0952\n",
      "Epoch 56/158\n",
      "7s - loss: 0.0948\n",
      "Epoch 57/158\n",
      "7s - loss: 0.0919\n",
      "Epoch 58/158\n",
      "7s - loss: 0.0941\n",
      "Epoch 59/158\n",
      "7s - loss: 0.0948\n",
      "Epoch 60/158\n",
      "7s - loss: 0.0906\n",
      "Epoch 61/158\n",
      "7s - loss: 0.0916\n",
      "Epoch 62/158\n",
      "7s - loss: 0.0917\n",
      "Epoch 63/158\n",
      "7s - loss: 0.0921\n",
      "Epoch 64/158\n",
      "7s - loss: 0.0928\n",
      "Epoch 65/158\n",
      "7s - loss: 0.0952\n",
      "Epoch 66/158\n",
      "7s - loss: 0.0922\n",
      "Epoch 67/158\n",
      "7s - loss: 0.0937\n",
      "Epoch 68/158\n",
      "7s - loss: 0.0916\n",
      "Epoch 69/158\n",
      "7s - loss: 0.0939\n",
      "Epoch 70/158\n",
      "7s - loss: 0.0900\n",
      "Epoch 71/158\n",
      "7s - loss: 0.0946\n",
      "Epoch 72/158\n",
      "7s - loss: 0.0901\n",
      "Epoch 73/158\n",
      "7s - loss: 0.0927\n",
      "Epoch 74/158\n",
      "7s - loss: 0.0930\n",
      "Epoch 75/158\n",
      "7s - loss: 0.0919\n",
      "Epoch 76/158\n",
      "7s - loss: 0.0914\n",
      "Epoch 77/158\n",
      "7s - loss: 0.0919\n",
      "Epoch 78/158\n",
      "7s - loss: 0.0938\n",
      "Epoch 79/158\n",
      "7s - loss: 0.0911\n",
      "Epoch 80/158\n",
      "7s - loss: 0.0904\n",
      "Epoch 81/158\n",
      "7s - loss: 0.0894\n",
      "Epoch 82/158\n",
      "7s - loss: 0.0945\n",
      "Epoch 83/158\n",
      "7s - loss: 0.0889\n",
      "Epoch 84/158\n",
      "7s - loss: 0.0893\n",
      "Epoch 85/158\n",
      "7s - loss: 0.0888\n",
      "Epoch 86/158\n",
      "7s - loss: 0.0914\n",
      "Epoch 87/158\n",
      "7s - loss: 0.0894\n",
      "Epoch 88/158\n",
      "7s - loss: 0.0914\n",
      "Epoch 89/158\n",
      "7s - loss: 0.0906\n",
      "Epoch 90/158\n",
      "7s - loss: 0.0880\n",
      "Epoch 91/158\n",
      "7s - loss: 0.0927\n",
      "Epoch 92/158\n",
      "7s - loss: 0.0909\n",
      "Epoch 93/158\n",
      "7s - loss: 0.0872\n",
      "Epoch 94/158\n",
      "7s - loss: 0.0887\n",
      "Epoch 95/158\n",
      "7s - loss: 0.0893\n",
      "Epoch 96/158\n",
      "7s - loss: 0.0908\n",
      "Epoch 97/158\n",
      "7s - loss: 0.0913\n",
      "Epoch 98/158\n",
      "7s - loss: 0.0896\n",
      "Epoch 99/158\n",
      "7s - loss: 0.0886\n",
      "Epoch 100/158\n",
      "7s - loss: 0.0882\n",
      "Epoch 101/158\n",
      "7s - loss: 0.0877\n",
      "Epoch 102/158\n",
      "7s - loss: 0.0888\n",
      "Epoch 103/158\n",
      "7s - loss: 0.0883\n",
      "Epoch 104/158\n",
      "7s - loss: 0.0892\n",
      "Epoch 105/158\n",
      "7s - loss: 0.0883\n",
      "Epoch 106/158\n",
      "7s - loss: 0.0893\n",
      "Epoch 107/158\n",
      "7s - loss: 0.0894\n",
      "Epoch 108/158\n",
      "7s - loss: 0.0894\n",
      "Epoch 109/158\n",
      "7s - loss: 0.0904\n",
      "Epoch 110/158\n",
      "7s - loss: 0.0875\n",
      "Epoch 111/158\n",
      "7s - loss: 0.0882\n",
      "Epoch 112/158\n",
      "7s - loss: 0.0889\n",
      "Epoch 113/158\n",
      "7s - loss: 0.0875\n",
      "Epoch 114/158\n",
      "7s - loss: 0.0885\n",
      "Epoch 115/158\n",
      "7s - loss: 0.0878\n",
      "Epoch 116/158\n",
      "7s - loss: 0.0885\n",
      "Epoch 117/158\n",
      "7s - loss: 0.0880\n",
      "Epoch 118/158\n",
      "7s - loss: 0.0888\n",
      "Epoch 119/158\n",
      "7s - loss: 0.0882\n",
      "Epoch 120/158\n",
      "7s - loss: 0.0872\n",
      "Epoch 121/158\n",
      "7s - loss: 0.0897\n",
      "Epoch 122/158\n",
      "7s - loss: 0.0894\n",
      "Epoch 123/158\n",
      "7s - loss: 0.0855\n",
      "Epoch 124/158\n",
      "7s - loss: 0.0889\n",
      "Epoch 125/158\n",
      "7s - loss: 0.0868\n",
      "Epoch 126/158\n",
      "7s - loss: 0.0900\n",
      "Epoch 127/158\n",
      "7s - loss: 0.0867\n",
      "Epoch 128/158\n",
      "7s - loss: 0.0882\n",
      "Epoch 129/158\n",
      "7s - loss: 0.0855\n",
      "Epoch 130/158\n",
      "7s - loss: 0.0910\n",
      "Epoch 131/158\n",
      "7s - loss: 0.0871\n",
      "Epoch 132/158\n",
      "7s - loss: 0.0868\n",
      "Epoch 133/158\n",
      "7s - loss: 0.0857\n",
      "Epoch 134/158\n",
      "7s - loss: 0.0870\n",
      "Epoch 135/158\n",
      "7s - loss: 0.0887\n",
      "Epoch 136/158\n",
      "7s - loss: 0.0877\n",
      "Epoch 137/158\n",
      "7s - loss: 0.0875\n",
      "Epoch 138/158\n",
      "7s - loss: 0.0857\n",
      "Epoch 139/158\n",
      "7s - loss: 0.0839\n",
      "Epoch 140/158\n",
      "7s - loss: 0.0884\n",
      "Epoch 141/158\n",
      "7s - loss: 0.0892\n",
      "Epoch 142/158\n",
      "7s - loss: 0.0848\n",
      "Epoch 143/158\n",
      "7s - loss: 0.0875\n",
      "Epoch 144/158\n",
      "7s - loss: 0.0885\n",
      "Epoch 145/158\n",
      "7s - loss: 0.0866\n",
      "Epoch 146/158\n",
      "7s - loss: 0.0881\n",
      "Epoch 147/158\n",
      "7s - loss: 0.0877\n",
      "Epoch 148/158\n",
      "7s - loss: 0.0866\n",
      "Epoch 149/158\n",
      "7s - loss: 0.0879\n",
      "Epoch 150/158\n",
      "7s - loss: 0.0867\n",
      "Epoch 151/158\n",
      "7s - loss: 0.0856\n",
      "Epoch 152/158\n",
      "7s - loss: 0.0883\n",
      "Epoch 153/158\n",
      "7s - loss: 0.0885\n",
      "Epoch 154/158\n",
      "7s - loss: 0.0858\n",
      "Epoch 155/158\n",
      "7s - loss: 0.0868\n",
      "Epoch 156/158\n",
      "7s - loss: 0.0874\n",
      "Epoch 157/158\n",
      "7s - loss: 0.0881\n",
      "Epoch 158/158\n",
      "7s - loss: 0.0887\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "filename_write = os.path.join(path,\"submit.csv\")\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "test_ids = df_test['id']\n",
    "\n",
    "# Preprocess\n",
    "encode(df_train)\n",
    "encode(df_test)\n",
    "\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x_train,y_train = to_xy(df_train,'sales')\n",
    "x_test = df_test.as_matrix().astype(np.float32)\n",
    "\n",
    "# Fit on entire training\n",
    "model = build_network(x_train)\n",
    "model.fit(x_train,y_train,batch_size=BATCH_SIZE,verbose=2,epochs=MAX_EPOCHS)\n",
    "    \n",
    "# Predict\n",
    "pred = model.predict(x_test)\n",
    "    \n",
    "# Create submission data set\n",
    "df_submit = pd.DataFrame(pred)\n",
    "df_submit.insert(0,'id',test_ids)\n",
    "df_submit.columns = ['id','sales']\n",
    "\n",
    "df_submit.to_csv(filename_write, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
